{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-21 13:39:24.858246: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-21 13:39:24.922345: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-21 13:39:24.945069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742564364.969542   54822 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742564364.985063   54822 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742564365.030722   54822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742564365.030749   54822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742564365.030752   54822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742564365.030753   54822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-21 13:39:25.041133: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-21 13:39:30.737269: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Insurance Cost Predictor initialized\n",
      "üöÄ Starting Insurance Cost Prediction Pipeline...\n",
      "üìÇ Loading dataset...\n",
      "üìä Dataset shape: (6788, 34)\n",
      "üî¢ Features: 33\n",
      "üë• Samples: 6788\n",
      "‚úÖ No missing values detected\n",
      "üìâ Target 'Insurance Cost' statistics:\n",
      "   - Mean: -0.00\n",
      "   - Median: -0.39\n",
      "   - Min: -1.17\n",
      "   - Max: 3.30\n",
      "üìÑ Loading selected features from selected_features.csv...\n",
      "‚úÖ Loaded 5 RFECV-selected features\n",
      "\n",
      "üìã Selected Features:\n",
      "   1. Smoking Status (Health)\n",
      "   2. Hypertension (Health)\n",
      "   3. Age (Demographic)\n",
      "   4. Savings Amount (Financial)\n",
      "   5. BMI (Health)\n",
      "\n",
      "üîß Preprocessing data...\n",
      "   - Engineering features...\n",
      "     ‚úì Added BMI √ó Smoking Status interaction\n",
      "     ‚úì Added Age √ó Hypertension interaction\n",
      "     ‚úì Created Age groups\n",
      "     ‚úì Created BMI categories\n",
      "   - Preparing features and target...\n",
      "   - Splitting data into train/validation/test sets...\n",
      "     ‚úì Training set: 4072 samples\n",
      "     ‚úì Validation set: 1358 samples\n",
      "     ‚úì Test set: 1358 samples\n",
      "   - Identified 2 categorical and 7 numerical features\n",
      "   - Fitting preprocessing pipeline...\n",
      "‚úÖ Data preprocessing complete\n",
      "\n",
      "üõ†Ô∏è Building models...\n",
      "\n",
      "üî® Building ElasticNet linear model...\n",
      "   - Starting ElasticNet hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Grid Search Progress:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Grid Search Progress:   0%|          | 0/16 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Best parameters: {'alpha': 0.001, 'l1_ratio': 0.1}\n",
      "   - Best CV score: 0.00 (MAE)\n",
      "‚úÖ ElasticNet model built with validation MAE: 0.00, RMSE: 0.00, R¬≤: 1.0000\n",
      "\n",
      "üî® Building Random Forest model...\n",
      "   - Starting Random Forest hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Grid Search Progress:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "# üß† Complete Insurance Cost Prediction Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Disable GPU if CUDA errors occur\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# For nice-looking progress bars in the notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('model_results', exist_ok=True)\n",
    "\n",
    "class InsuranceCostPredictor:\n",
    "    def __init__(self, data_path, model_dir='model_results'):\n",
    "        \"\"\"Initialize the predictor with data path and model directory.\"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.model_dir = model_dir\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.feature_importance = {}\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "        self.best_score = float('inf')  # Lower is better for MAE\n",
    "        \n",
    "        # Define feature categories\n",
    "        self.demographic_features = [\"Age\", \"Gender\", \"Region\", \"Number of Dependents\"]\n",
    "        self.health_features = [\"BMI\", \"Smoking Status\", \"Diabetes\", \"Hypertension\", \n",
    "                               \"Heart Disease\", \"Cancer History\", \"Stroke\", \"Liver Disease\", \n",
    "                               \"Kidney Disease\", \"COPD\", \"TB\", \"HIV/AIDS\", \n",
    "                               \"Alcohol Consumption\", \"Exercise Frequency\", \"Diet Type\", \n",
    "                               \"Stress Level\", \"Medical History Score\", \"Hospital Visits Per Year\"]\n",
    "        self.financial_features = [\"Annual Income\", \"Employment Type\", \"Credit Score\", \n",
    "                                  \"Savings Amount\", \"Previous Insurance Claims\", \n",
    "                                  \"Policy Type\", \"Policy Renewal Status\", \"Medication Costs Per Year\"]\n",
    "        \n",
    "        print(\"üîç Insurance Cost Predictor initialized\")\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and explore the dataset.\"\"\"\n",
    "        print(\"üìÇ Loading dataset...\")\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "        \n",
    "        # Basic exploration\n",
    "        print(f\"üìä Dataset shape: {self.df.shape}\")\n",
    "        print(f\"üî¢ Features: {self.df.shape[1]-1}\")\n",
    "        print(f\"üë• Samples: {self.df.shape[0]}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing = self.df.isnull().sum()\n",
    "        if missing.sum() > 0:\n",
    "            print(\"‚ö†Ô∏è Missing values detected:\")\n",
    "            print(missing[missing > 0])\n",
    "            \n",
    "            # Fill missing numerical values with median\n",
    "            for col in self.df.select_dtypes(include=['float64', 'int64']):\n",
    "                self.df[col] = self.df[col].fillna(self.df[col].median())\n",
    "            \n",
    "            # Fill missing categorical values with mode\n",
    "            for col in self.df.select_dtypes(include=['object']):\n",
    "                self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n",
    "                \n",
    "            print(\"‚úÖ Missing values handled\")\n",
    "        else:\n",
    "            print(\"‚úÖ No missing values detected\")\n",
    "        \n",
    "        # Target variable statistics\n",
    "        target = 'Insurance Cost'\n",
    "        print(f\"üìâ Target '{target}' statistics:\")\n",
    "        print(f\"   - Mean: {self.df[target].mean():.2f}\")\n",
    "        print(f\"   - Median: {self.df[target].median():.2f}\")\n",
    "        print(f\"   - Min: {self.df[target].min():.2f}\")\n",
    "        print(f\"   - Max: {self.df[target].max():.2f}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def load_selected_features(self, features_path=None, top_n=10, use_rfecv=True):\n",
    "        \"\"\"Load features from CSV or use specific features.\"\"\"\n",
    "        if features_path:\n",
    "            print(f\"üìÑ Loading selected features from {features_path}...\")\n",
    "            features_df = pd.read_csv(features_path)\n",
    "            \n",
    "            # If RFECV is preferred, use only those features\n",
    "            if use_rfecv:\n",
    "                self.selected_features = features_df[features_df['RFECV_Selected'] == 1]['Feature'].tolist()\n",
    "                print(f\"‚úÖ Loaded {len(self.selected_features)} RFECV-selected features\")\n",
    "            else:\n",
    "                # Take top N features\n",
    "                self.selected_features = features_df.head(top_n)['Feature'].tolist()\n",
    "                print(f\"‚úÖ Loaded top {top_n} features\")\n",
    "        else:\n",
    "            # Use default important features based on previous analysis\n",
    "            self.selected_features = ['Smoking Status', 'Hypertension', 'Age', 'BMI', 'Savings Amount']\n",
    "            print(f\"‚úÖ Using default top 5 features\")\n",
    "            \n",
    "        # Add 'Name' to excluded features to make sure it's not used\n",
    "        self.excluded_features = ['Name', 'Insurance Cost', 'BMI Smoker']\n",
    "        \n",
    "        # Print selected features with categories\n",
    "        print(\"\\nüìã Selected Features:\")\n",
    "        for i, feature in enumerate(self.selected_features, 1):\n",
    "            if feature in self.demographic_features:\n",
    "                category = \"Demographic\"\n",
    "            elif feature in self.health_features:\n",
    "                category = \"Health\"\n",
    "            elif feature in self.financial_features:\n",
    "                category = \"Financial\"\n",
    "            else:\n",
    "                category = \"Other\"\n",
    "            print(f\"   {i}. {feature} ({category})\")\n",
    "            \n",
    "        return self.selected_features\n",
    "    \n",
    "    def preprocess(self):\n",
    "        \"\"\"Preprocess the data for modeling.\"\"\"\n",
    "        print(\"\\nüîß Preprocessing data...\")\n",
    "        \n",
    "        # Create feature engineering pipeline\n",
    "        print(\"   - Engineering features...\")\n",
    "        \n",
    "        # Add interaction terms\n",
    "        if 'BMI' in self.selected_features and 'Smoking Status' in self.selected_features:\n",
    "            self.df['BMI_Smoking'] = self.df['BMI'] * self.df['Smoking Status']\n",
    "            self.selected_features.append('BMI_Smoking')\n",
    "            print(\"     ‚úì Added BMI √ó Smoking Status interaction\")\n",
    "            \n",
    "        if 'Age' in self.selected_features and 'Hypertension' in self.selected_features:\n",
    "            self.df['Age_Hypertension'] = self.df['Age'] * self.df['Hypertension']\n",
    "            self.selected_features.append('Age_Hypertension')\n",
    "            print(\"     ‚úì Added Age √ó Hypertension interaction\")\n",
    "            \n",
    "        # Log transform skewed numerical features\n",
    "        skewed_features = ['Savings Amount'] \n",
    "        skewed_features = [f for f in skewed_features if f in self.selected_features]\n",
    "        \n",
    "        for feature in skewed_features:\n",
    "            if (self.df[feature] > 0).all():  # Only transform positive values\n",
    "                self.df[f'{feature}_Log'] = np.log1p(self.df[feature])\n",
    "                self.selected_features.append(f'{feature}_Log')\n",
    "                print(f\"     ‚úì Log-transformed {feature}\")\n",
    "        \n",
    "        # Age groups (0-18, 19-35, 36-50, 51-65, 65+)\n",
    "        if 'Age' in self.selected_features:\n",
    "            self.df['Age_Group'] = pd.cut(\n",
    "                self.df['Age'], \n",
    "                bins=[0, 18, 35, 50, 65, 100], \n",
    "                labels=['0-18', '19-35', '36-50', '51-65', '65+']\n",
    "            )\n",
    "            self.selected_features.append('Age_Group')\n",
    "            print(\"     ‚úì Created Age groups\")\n",
    "        \n",
    "        # BMI categories (Underweight, Normal, Overweight, Obese)\n",
    "        if 'BMI' in self.selected_features:\n",
    "            self.df['BMI_Category'] = pd.cut(\n",
    "                self.df['BMI'], \n",
    "                bins=[0, 18.5, 25, 30, 100], \n",
    "                labels=['Underweight', 'Normal', 'Overweight', 'Obese']\n",
    "            )\n",
    "            self.selected_features.append('BMI_Category')\n",
    "            print(\"     ‚úì Created BMI categories\")\n",
    "        \n",
    "        # Prepare features and target\n",
    "        print(\"   - Preparing features and target...\")\n",
    "        X = self.df[self.selected_features].copy()\n",
    "        y = self.df['Insurance Cost']\n",
    "        \n",
    "        # Split the data\n",
    "        print(\"   - Splitting data into train/validation/test sets...\")\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val, test_size=0.25, random_state=42\n",
    "        )  # 0.25 of 0.8 = 0.2 of overall data\n",
    "        \n",
    "        print(f\"     ‚úì Training set: {X_train.shape[0]} samples\")\n",
    "        print(f\"     ‚úì Validation set: {X_val.shape[0]} samples\")\n",
    "        print(f\"     ‚úì Test set: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        # Identify categorical features\n",
    "        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        \n",
    "        print(f\"   - Identified {len(categorical_features)} categorical and {len(numeric_features)} numerical features\")\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numeric_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "        \n",
    "        # Fit the preprocessor on the training data\n",
    "        print(\"   - Fitting preprocessing pipeline...\")\n",
    "        preprocessor.fit(X_train)\n",
    "        \n",
    "        # Transform the data\n",
    "        X_train_processed = preprocessor.transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "        X_test_processed = preprocessor.transform(X_test)\n",
    "        \n",
    "        # Save the data splits\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        \n",
    "        # Save the processed data\n",
    "        self.X_train_processed = X_train_processed\n",
    "        self.X_val_processed = X_val_processed\n",
    "        self.X_test_processed = X_test_processed\n",
    "        \n",
    "        # Save the preprocessor\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "        print(\"‚úÖ Data preprocessing complete\")\n",
    "        return X_train_processed, y_train, X_val_processed, y_val, X_test_processed, y_test\n",
    "    \n",
    "    def build_linear_model(self):\n",
    "        \"\"\"Build ElasticNet linear model.\"\"\"\n",
    "        print(\"\\nüî® Building ElasticNet linear model...\")\n",
    "        \n",
    "        param_grid = {\n",
    "            'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "            'l1_ratio': [0.1, 0.5, 0.7, 0.9]\n",
    "        }\n",
    "        \n",
    "        elasticnet = ElasticNet(random_state=42, max_iter=2000)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=elasticnet,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"   - Starting ElasticNet hyperparameter tuning...\")\n",
    "        with tqdm(total=len(param_grid['alpha']) * len(param_grid['l1_ratio']), \n",
    "                  desc=\"   - Grid Search Progress\") as pbar:\n",
    "            # Define callback class to update progress bar\n",
    "            class TqdmCallback:\n",
    "                def __init__(self, pbar):\n",
    "                    self.pbar = pbar\n",
    "                    self.count = 0\n",
    "                    \n",
    "                def __call__(self, model, step=None):\n",
    "                    self.count += 1\n",
    "                    self.pbar.update()\n",
    "            \n",
    "            # Run grid search\n",
    "            grid_search.fit(self.X_train_processed, self.y_train)\n",
    "        \n",
    "        # Get the best model\n",
    "        best_elasticnet = grid_search.best_estimator_\n",
    "        \n",
    "        # Save the model\n",
    "        self.models['ElasticNet'] = best_elasticnet\n",
    "        \n",
    "        print(f\"   - Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"   - Best CV score: {-grid_search.best_score_:.2f} (MAE)\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_elasticnet.predict(self.X_val_processed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(self.y_val, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_val, y_pred))\n",
    "        r2 = r2_score(self.y_val, y_pred)\n",
    "        \n",
    "        # Save results\n",
    "        self.results['ElasticNet'] = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ ElasticNet model built with validation MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.4f}\")\n",
    "        return best_elasticnet\n",
    "        \n",
    "    def build_rf_model(self):\n",
    "        \"\"\"Build Random Forest model.\"\"\"\n",
    "        print(\"\\nüî® Building Random Forest model...\")\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "        \n",
    "        rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=rf,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"   - Starting Random Forest hyperparameter tuning...\")\n",
    "        with tqdm(total=len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']), \n",
    "                  desc=\"   - Grid Search Progress\") as pbar:\n",
    "            class TqdmCallback:\n",
    "                def __init__(self, pbar):\n",
    "                    self.pbar = pbar\n",
    "                    self.count = 0\n",
    "                    \n",
    "                def __call__(self, model, step=None):\n",
    "                    self.count += 1\n",
    "                    self.pbar.update()\n",
    "            \n",
    "            # Run grid search\n",
    "            grid_search.fit(self.X_train_processed, self.y_train)\n",
    "        \n",
    "        # Get the best model\n",
    "        best_rf = grid_search.best_estimator_\n",
    "        \n",
    "        # Save the model\n",
    "        self.models['RandomForest'] = best_rf\n",
    "        \n",
    "        print(f\"   - Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"   - Best CV score: {-grid_search.best_score_:.2f} (MAE)\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_rf.predict(self.X_val_processed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(self.y_val, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_val, y_pred))\n",
    "        r2 = r2_score(self.y_val, y_pred)\n",
    "        \n",
    "        # Save results\n",
    "        self.results['RandomForest'] = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "        \n",
    "        # Feature importance\n",
    "        if hasattr(best_rf, 'feature_importances_'):\n",
    "            # Get feature names after preprocessing\n",
    "            feature_names = []\n",
    "            for name, transformer, features in self.preprocessor.transformers_:\n",
    "                if hasattr(transformer, 'get_feature_names_out'):\n",
    "                    feature_names.extend(transformer.get_feature_names_out(features))\n",
    "                else:\n",
    "                    feature_names.extend(features)\n",
    "            \n",
    "            # Limit to length of feature_importances_\n",
    "            feature_names = feature_names[:len(best_rf.feature_importances_)]\n",
    "            \n",
    "            # Create DataFrame of feature importances\n",
    "            importances = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': best_rf.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            self.feature_importance['RandomForest'] = importances\n",
    "        \n",
    "        print(f\"‚úÖ Random Forest model built with validation MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.4f}\")\n",
    "        return best_rf\n",
    "    \n",
    "    def build_xgb_model(self):\n",
    "        \"\"\"Build XGBoost model.\"\"\"\n",
    "        print(\"\\nüî® Building XGBoost model...\")\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "        \n",
    "        xgb = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=xgb,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"   - Starting XGBoost hyperparameter tuning...\")\n",
    "        with tqdm(total=len(param_grid['n_estimators']) * len(param_grid['max_depth']) * \n",
    "                 len(param_grid['learning_rate']) * len(param_grid['subsample']), \n",
    "                 desc=\"   - Grid Search Progress\") as pbar:\n",
    "            class TqdmCallback:\n",
    "                def __init__(self, pbar):\n",
    "                    self.pbar = pbar\n",
    "                    self.count = 0\n",
    "                    \n",
    "                def __call__(self, model, step=None):\n",
    "                    self.count += 1\n",
    "                    self.pbar.update()\n",
    "            \n",
    "            # Run grid search\n",
    "            grid_search.fit(self.X_train_processed, self.y_train)\n",
    "        \n",
    "        # Get the best model\n",
    "        best_xgb = grid_search.best_estimator_\n",
    "        \n",
    "        # Save the model\n",
    "        self.models['XGBoost'] = best_xgb\n",
    "        \n",
    "        print(f\"   - Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"   - Best CV score: {-grid_search.best_score_:.2f} (MAE)\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_xgb.predict(self.X_val_processed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(self.y_val, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_val, y_pred))\n",
    "        r2 = r2_score(self.y_val, y_pred)\n",
    "        \n",
    "        # Save results\n",
    "        self.results['XGBoost'] = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "        \n",
    "        # Feature importance\n",
    "        if hasattr(best_xgb, 'feature_importances_'):\n",
    "            # Get feature names after preprocessing\n",
    "            feature_names = []\n",
    "            for name, transformer, features in self.preprocessor.transformers_:\n",
    "                if hasattr(transformer, 'get_feature_names_out'):\n",
    "                    feature_names.extend(transformer.get_feature_names_out(features))\n",
    "                else:\n",
    "                    feature_names.extend(features)\n",
    "            \n",
    "            # Limit to length of feature_importances_\n",
    "            feature_names = feature_names[:len(best_xgb.feature_importances_)]\n",
    "            \n",
    "            # Create DataFrame of feature importances\n",
    "            importances = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': best_xgb.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            self.feature_importance['XGBoost'] = importances\n",
    "        \n",
    "        print(f\"‚úÖ XGBoost model built with validation MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.4f}\")\n",
    "        return best_xgb\n",
    "    \n",
    "    def build_nn_model(self):\n",
    "        \"\"\"Build Neural Network model.\"\"\"\n",
    "        print(\"\\nüî® Building Neural Network model...\")\n",
    "        \n",
    "        # Define architectures to try\n",
    "        architectures = [\n",
    "            # [Dense units, dropout rate]\n",
    "            [[64, 32], [0.2, 0.2]],\n",
    "            [[128, 64, 32], [0.3, 0.2, 0.1]],\n",
    "            [[64, 64, 32, 16], [0.3, 0.3, 0.2, 0.1]]\n",
    "        ]\n",
    "        \n",
    "        learning_rates = [0.001, 0.0005]\n",
    "        batch_sizes = [32, 64]\n",
    "        \n",
    "        # Create early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=0.00001,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        best_val_mae = float('inf')\n",
    "        best_architecture = None\n",
    "        best_lr = None\n",
    "        best_batch_size = None\n",
    "        best_history = None\n",
    "        \n",
    "        # Get input shape from processed data\n",
    "        input_shape = self.X_train_processed.shape[1]\n",
    "        \n",
    "        total_combinations = len(architectures) * len(learning_rates) * len(batch_sizes)\n",
    "        pbar = tqdm(total=total_combinations, desc=\"   - Testing NN configurations\")\n",
    "        \n",
    "        for arch_idx, (units, dropouts) in enumerate(architectures):\n",
    "            for lr in learning_rates:\n",
    "                for batch_size in batch_sizes:\n",
    "                    # Build model\n",
    "                    model = Sequential()\n",
    "                    \n",
    "                    # Add input layer\n",
    "                    model.add(Dense(units[0], activation='relu', input_shape=(input_shape,)))\n",
    "                    model.add(Dropout(dropouts[0]))\n",
    "                    \n",
    "                    # Add hidden layers\n",
    "                    for i in range(1, len(units)):\n",
    "                        model.add(Dense(units[i], activation='relu'))\n",
    "                        model.add(Dropout(dropouts[i]))\n",
    "                    \n",
    "                    # Add output layer\n",
    "                    model.add(Dense(1))  # Linear activation for regression\n",
    "                    \n",
    "                    # Compile model\n",
    "                    model.compile(\n",
    "                        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                        loss=tf.keras.losses.Huber(),  # Use Huber loss (robust to outliers)\n",
    "                        metrics=['mae']\n",
    "                    )\n",
    "                    \n",
    "                    # Train model\n",
    "                    history = model.fit(\n",
    "                        self.X_train_processed, self.y_train,\n",
    "                        validation_data=(self.X_val_processed, self.y_val),\n",
    "                        epochs=100,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping, reduce_lr],\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    # Evaluate model\n",
    "                    val_loss, val_mae = model.evaluate(\n",
    "                        self.X_val_processed, self.y_val, \n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    # Update progress bar with result\n",
    "                    pbar.set_postfix({'val_mae': val_mae})\n",
    "                    pbar.update()\n",
    "                    \n",
    "                    # Check if this is the best model\n",
    "                    if val_mae < best_val_mae:\n",
    "                        best_val_mae = val_mae\n",
    "                        best_architecture = (units, dropouts)\n",
    "                        best_lr = lr\n",
    "                        best_batch_size = batch_size\n",
    "                        best_history = history.history\n",
    "                        \n",
    "                        # Save the best model\n",
    "                        self.models['NeuralNetwork'] = model\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Build final model with best parameters\n",
    "        print(f\"\\n   - Best architecture: {best_architecture[0]}\")\n",
    "        print(f\"   - Best learning rate: {best_lr}\")\n",
    "        print(f\"   - Best batch size: {best_batch_size}\")\n",
    "        \n",
    "        # Get the best model\n",
    "        best_nn = self.models.get('NeuralNetwork')\n",
    "        \n",
    "        if best_nn is not None:\n",
    "            # Make predictions\n",
    "            y_pred = best_nn.predict(self.X_val_processed, verbose=0)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(self.y_val, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(self.y_val, y_pred))\n",
    "            r2 = r2_score(self.y_val, y_pred)\n",
    "            \n",
    "            # Save results\n",
    "            self.results['NeuralNetwork'] = {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'best_params': {\n",
    "                    'architecture': best_architecture,\n",
    "                    'learning_rate': best_lr,\n",
    "                    'batch_size': best_batch_size\n",
    "                },\n",
    "                'history': best_history\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Neural Network model built with validation MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.4f}\")\n",
    "            return best_nn\n",
    "        else:\n",
    "            print(\"‚ùå Neural Network model build failed\")\n",
    "            return None\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare the performance of all models.\"\"\"\n",
    "        print(\"\\nüìä Model Comparison:\")\n",
    "        \n",
    "        # Create a DataFrame with results\n",
    "        results_df = pd.DataFrame({\n",
    "            'Model': list(self.results.keys()),\n",
    "            'MAE': [self.results[model]['mae'] for model in self.results],\n",
    "            'RMSE': [self.results[model]['rmse'] for model in self.results],\n",
    "            'R¬≤': [self.results[model]['r2'] for model in self.results]\n",
    "        })\n",
    "        \n",
    "        # Sort by MAE (lower is better)\n",
    "        results_df = results_df.sort_values('MAE')\n",
    "        \n",
    "        # Print results\n",
    "        print(results_df)\n",
    "        \n",
    "        # Identify best model\n",
    "        best_model_name = results_df.iloc[0]['Model']\n",
    "        self.best_model_name = best_model_name\n",
    "        self.best_model = self.models[best_model_name]\n",
    "        self.best_score = results_df.iloc[0]['MAE']\n",
    "        \n",
    "        print(f\"\\nüèÜ Best model: {best_model_name} with MAE: {self.best_score:.2f}\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Visualize model performance.\"\"\"\n",
    "        print(\"\\nüìà Visualizing model results...\")\n",
    "        \n",
    "        # Create a DataFrame with results\n",
    "        results_df = pd.DataFrame({\n",
    "            'Model': list(self.results.keys()),\n",
    "            'MAE': [self.results[model]['mae'] for model in self.results],\n",
    "            'RMSE': [self.results[model]['rmse'] for model in self.results],\n",
    "            'R¬≤': [self.results[model]['r2'] for model in self.results]\n",
    "        })\n",
    "        \n",
    "        # Sort by MAE (lower is better)\n",
    "        results_df = results_df.sort_values('MAE')\n",
    "        \n",
    "        # Create a bar chart of model performance\n",
    "        fig = make_subplots(rows=1, cols=3, \n",
    "                           subplot_titles=(\"Mean Absolute Error\", \"Root Mean Squared Error\", \"R¬≤ Score\"),\n",
    "                           shared_yaxes=True)\n",
    "        \n",
    "        # Add bars for each metric\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=results_df['Model'], y=results_df['MAE'], name='MAE',\n",
    "                   text=results_df['MAE'].round(2), textposition='auto',\n",
    "                   marker_color='crimson'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=results_df['Model'], y=results_df['RMSE'], name='RMSE',\n",
    "                   text=results_df['RMSE'].round(2), textposition='auto',\n",
    "                   marker_color='darkorange'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=results_df['Model'], y=results_df['R¬≤'], name='R¬≤',\n",
    "                   text=results_df['R¬≤'].round(4), textposition='auto',\n",
    "                   marker_color='teal'),\n",
    "            row=1, col=3\n",
    "        )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title='Model Performance Comparison',\n",
    "            height=500,\n",
    "            width=1000,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "        \n",
    "        # If Neural Network is in the models, plot training history\n",
    "        if 'NeuralNetwork' in self.results and 'history' in self.results['NeuralNetwork']:\n",
    "            history = self.results['NeuralNetwork']['history']\n",
    "            \n",
    "            # Create a line plot of training history\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=list(range(len(history['loss']))), y=history['loss'],\n",
    "                          name='Training Loss', line=dict(color='blue'))\n",
    "            )\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=list(range(len(history['val_loss']))), y=history['val_loss'],\n",
    "                          name='Validation Loss', line=dict(color='red'))\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title='Neural Network Training History',\n",
    "                xaxis_title='Epochs',\n",
    "                yaxis_title='Loss',\n",
    "                height=400,\n",
    "                width=800\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "        # Plot feature importances if available\n",
    "        if self.feature_importance:\n",
    "            model_with_importances = next(iter(self.feature_importance.keys()))\n",
    "            importances = self.feature_importance[model_with_importances]\n",
    "            \n",
    "            # Take top 15 features\n",
    "            importances = importances.head(15)\n",
    "            \n",
    "            fig = px.bar(\n",
    "                importances, \n",
    "                x='Importance', \n",
    "                y='Feature',\n",
    "                orientation='h',\n",
    "                title=f'Top 15 Feature Importances ({model_with_importances})',\n",
    "                color='Importance',\n",
    "                color_continuous_scale='teal'\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                height=500, \n",
    "                width=800,\n",
    "                yaxis=dict(autorange=\"reversed\")\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "        \n",
    "        # Actual vs. Predicted plot for best model\n",
    "        if self.best_model_name:\n",
    "            # Get predictions\n",
    "            if self.best_model_name == 'NeuralNetwork':\n",
    "                y_pred = self.models[self.best_model_name].predict(self.X_test_processed, verbose=0)\n",
    "            else:\n",
    "                y_pred = self.models[self.best_model_name].predict(self.X_test_processed)\n",
    "            \n",
    "            # Create a scatter plot of actual vs. predicted\n",
    "            fig = px.scatter(\n",
    "                x=self.y_test, \n",
    "                y=y_pred.flatten(),\n",
    "                title=f'Actual vs. Predicted Insurance Cost ({self.best_model_name})',\n",
    "                labels={'x': 'Actual Cost', 'y': 'Predicted Cost'}\n",
    "            )\n",
    "            \n",
    "            # Add a 45-degree line\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[self.y_test.min(), self.y_test.max()], \n",
    "                    y=[self.y_test.min(), self.y_test.max()],\n",
    "                    mode='lines',\n",
    "                    name='45¬∞ Line',\n",
    "                    line=dict(color='red', dash='dash')\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(height=600, width=800)\n",
    "            fig.show()\n",
    "            \n",
    "            # Residual plot\n",
    "            residuals = self.y_test - y_pred.flatten()\n",
    "            fig = px.scatter(\n",
    "                x=y_pred.flatten(), \n",
    "                y=residuals,\n",
    "                title=f'Residual Plot ({self.best_model_name})',\n",
    "                labels={'x': 'Predicted Cost', 'y': 'Residuals'}\n",
    "            )\n",
    "            \n",
    "            # Add a horizontal line at y=0\n",
    "            fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\")\n",
    "            \n",
    "            fig.update_layout(height=500, width=800)\n",
    "            fig.show()\n",
    "            \n",
    "            # Histogram of residuals\n",
    "            fig = px.histogram(\n",
    "                residuals, \n",
    "                nbins=50,\n",
    "                title=f'Distribution of Residuals ({self.best_model_name})'\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(height=400, width=800)\n",
    "            fig.show()\n",
    "    \n",
    "    def final_evaluation(self):\n",
    "        \"\"\"Evaluate the best model on the test set.\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"‚ùå No best model found. Run compare_models() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüß™ Final evaluation of {self.best_model_name} on test data...\")\n",
    "        \n",
    "        # Get predictions\n",
    "        if self.best_model_name == 'NeuralNetwork':\n",
    "            y_pred = self.best_model.predict(self.X_test_processed, verbose=0)\n",
    "        else:\n",
    "            y_pred = self.best_model.predict(self.X_test_processed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(self.y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "        \n",
    "        # Save test results\n",
    "        self.test_results = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2\n",
    "        }\n",
    "        \n",
    "        print(f\"üìä Test Set Results:\")\n",
    "        print(f\"   - MAE: {mae:.2f}\")\n",
    "        print(f\"   - RMSE: {rmse:.2f}\")\n",
    "        print(f\"   - R¬≤: {r2:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        model_path = f\"{self.model_dir}/{self.best_model_name}.pkl\"\n",
    "        if self.best_model_name != 'NeuralNetwork':\n",
    "            joblib.dump(self.best_model, model_path)\n",
    "            print(f\"‚úÖ Best model saved to {model_path}\")\n",
    "        else:\n",
    "            # Save neural network model\n",
    "            model_path = f\"{self.model_dir}/{self.best_model_name}.h5\"\n",
    "            self.best_model.save(model_path)\n",
    "            print(f\"‚úÖ Neural Network model saved to {model_path}\")\n",
    "        \n",
    "        # Save the preprocessor\n",
    "        preprocessor_path = f\"{self.model_dir}/preprocessor.pkl\"\n",
    "        joblib.dump(self.preprocessor, preprocessor_path)\n",
    "        print(f\"‚úÖ Preprocessor saved to {preprocessor_path}\")\n",
    "        \n",
    "        # Save results\n",
    "        results_path = f\"{self.model_dir}/results.csv\"\n",
    "        results_df = pd.DataFrame({\n",
    "            'Model': list(self.results.keys()) + ['Best Model (Test)'],\n",
    "            'MAE': [self.results[model]['mae'] for model in self.results] + [mae],\n",
    "            'RMSE': [self.results[model]['rmse'] for model in self.results] + [rmse],\n",
    "            'R¬≤': [self.results[model]['r2'] for model in self.results] + [r2]\n",
    "        })\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"‚úÖ Results saved to {results_path}\")\n",
    "        \n",
    "        return self.test_results\n",
    "    \n",
    "    def run_pipeline(self, features_path=None, top_n=10, use_rfecv=True):\n",
    "        \"\"\"Run the full modeling pipeline.\"\"\"\n",
    "        print(\"üöÄ Starting Insurance Cost Prediction Pipeline...\")\n",
    "        \n",
    "        # Step 1: Load data\n",
    "        self.load_data()\n",
    "        \n",
    "        # Step 2: Load selected features\n",
    "        self.load_selected_features(features_path, top_n, use_rfecv)\n",
    "        \n",
    "        # Step 3: Preprocess data\n",
    "        self.preprocess()\n",
    "        \n",
    "        # Step 4: Build models\n",
    "        print(\"\\nüõ†Ô∏è Building models...\")\n",
    "        self.build_linear_model()\n",
    "        self.build_rf_model()\n",
    "        self.build_xgb_model()\n",
    "        self.build_nn_model()\n",
    "        \n",
    "        # Step 5: Compare models\n",
    "        self.compare_models()\n",
    "        \n",
    "        # Step 6: Visualize results\n",
    "        self.visualize_results()\n",
    "        \n",
    "        # Step 7: Final evaluation\n",
    "        self.final_evaluation()\n",
    "        \n",
    "        print(\"\\n‚úÖ Pipeline complete!\")\n",
    "        return self.best_model_name, self.test_results\n",
    "\n",
    "    def export_all_models(self):\n",
    "        \"\"\"Export all trained models to disk.\"\"\"\n",
    "        print(\"\\nüíæ Exporting all trained models...\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "        \n",
    "        # Export each model\n",
    "        for model_name, model in self.models.items():\n",
    "            if model_name == 'NeuralNetwork':\n",
    "                # Save neural network model in h5 format\n",
    "                model_path = f\"{self.model_dir}/{model_name}.h5\"\n",
    "                model.save(model_path)\n",
    "            else:\n",
    "                # Save scikit-learn models with joblib\n",
    "                model_path = f\"{self.model_dir}/{model_name}.pkl\"\n",
    "                joblib.dump(model, model_path)\n",
    "            \n",
    "            # Print success message with metrics\n",
    "            if model_name in self.results:\n",
    "                mae = self.results[model_name]['mae']\n",
    "                rmse = self.results[model_name]['rmse']\n",
    "                r2 = self.results[model_name]['r2']\n",
    "                print(f\"‚úÖ {model_name} saved to {model_path} (MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.4f})\")\n",
    "            else:\n",
    "                print(f\"‚úÖ {model_name} saved to {model_path}\")\n",
    "        \n",
    "        # Also export the preprocessor separately (if not already done)\n",
    "        preprocessor_path = f\"{self.model_dir}/preprocessor.pkl\"\n",
    "        joblib.dump(self.preprocessor, preprocessor_path)\n",
    "        print(f\"‚úÖ Preprocessor saved to {preprocessor_path}\")\n",
    "        \n",
    "        return list(self.models.keys())\n",
    "# Run the pipeline\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = InsuranceCostPredictor(data_path=\"insurance_cleaned.csv\")\n",
    "    predictor.run_pipeline(features_path=\"selected_features.csv\", top_n=10, use_rfecv=True)\n",
    "    \n",
    "    # Export all models\n",
    "    predictor.export_all_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
