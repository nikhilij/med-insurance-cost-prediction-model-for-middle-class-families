{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Insurance Cost Predictor initialized\n",
      "🚀 Starting Insurance Cost Prediction Pipeline...\n",
      "📂 Loading dataset...\n",
      "📊 Dataset shape: (6788, 34)\n",
      "🔢 Features: 33\n",
      "👥 Samples: 6788\n",
      "✅ No missing values detected\n",
      "📉 Target 'Insurance Cost' statistics:\n",
      "   - Mean: -0.00\n",
      "   - Median: -0.39\n",
      "   - Min: -1.17\n",
      "   - Max: 3.30\n",
      "📄 Loading selected features from selected_features.csv...\n",
      "✅ Loaded 5 RFECV-selected features\n",
      "\n",
      "📋 Selected Features:\n",
      "   1. Smoking Status (Health)\n",
      "   2. Hypertension (Health)\n",
      "   3. Age (Demographic)\n",
      "   4. BMI (Health)\n",
      "   5. Savings Amount (Financial)\n",
      "\n",
      "🔧 Preprocessing data...\n",
      "   - Engineering features...\n",
      "     ✓ Added BMI × Smoking Status interaction\n",
      "     ✓ Added Age × Hypertension interaction\n",
      "     ✓ Created Age groups\n",
      "     ✓ Created BMI categories\n",
      "   - Preparing features and target...\n",
      "   - Splitting data into train/validation/test sets...\n",
      "     ✓ Training set: 4072 samples\n",
      "     ✓ Validation set: 1358 samples\n",
      "     ✓ Test set: 1358 samples\n",
      "   - Identified 2 categorical and 7 numerical features\n",
      "   - Fitting preprocessing pipeline...\n",
      "✅ Data preprocessing complete\n",
      "\n",
      "🛠️ Building models...\n",
      "\n",
      "🔨 Building ElasticNet linear model...\n",
      "   - Starting ElasticNet hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Grid Search Progress:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Grid Search Progress:   0%|          | 0/16 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Best parameters: {'alpha': 0.001, 'l1_ratio': 0.1}\n",
      "   - Best CV score: 0.00 (MAE)\n",
      "✅ ElasticNet model built with validation MAE: 0.00, RMSE: 0.00, R²: 1.0000\n",
      "\n",
      "🔨 Building Random Forest model...\n",
      "   - Starting Random Forest hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Grid Search Progress:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Grid Search Progress:   0%|          | 0/12 [01:03<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Best parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "   - Best CV score: 0.01 (MAE)\n",
      "✅ Random Forest model built with validation MAE: 0.00, RMSE: 0.01, R²: 0.9999\n",
      "\n",
      "🔨 Building XGBoost model...\n",
      "   - Starting XGBoost hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Grid Search Progress:   0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Grid Search Progress:   0%|          | 0/36 [00:20<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Best parameters: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 200, 'subsample': 0.8}\n",
      "   - Best CV score: 0.00 (MAE)\n",
      "✅ XGBoost model built with validation MAE: 0.00, RMSE: 0.01, R²: 1.0000\n",
      "\n",
      "🔨 Building Neural Network model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:   0%|          | 0/12 [00:00<?, ?it/s]/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 33: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:   8%|▊         | 1/12 [00:10<01:50, 10.06s/it, val_mae=0.0316]/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 48: early stopping\n",
      "Restoring model weights from the end of the best epoch: 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:  17%|█▋        | 2/12 [00:18<01:32,  9.27s/it, val_mae=0.0206]/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 36: early stopping\n",
      "Restoring model weights from the end of the best epoch: 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:  25%|██▌       | 3/12 [00:30<01:32, 10.27s/it, val_mae=0.0372]/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 42: early stopping\n",
      "Restoring model weights from the end of the best epoch: 22.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:  33%|███▎      | 4/12 [00:37<01:14,  9.26s/it, val_mae=0.063] /usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 24: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:  42%|████▏     | 5/12 [00:45<01:01,  8.82s/it, val_mae=0.105]/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:  50%|█████     | 6/12 [00:51<00:46,  7.82s/it, val_mae=0.0769]/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:  58%|█████▊    | 7/12 [01:00<00:40,  8.02s/it, val_mae=0.0743]/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:  67%|██████▋   | 8/12 [01:07<00:30,  7.60s/it, val_mae=0.0702]/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:  75%|███████▌  | 9/12 [01:15<00:23,  7.89s/it, val_mae=0.133] /usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:  83%|████████▎ | 10/12 [01:21<00:14,  7.30s/it, val_mae=0.131]/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 34: early stopping\n",
      "Restoring model weights from the end of the best epoch: 14.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   - Testing NN configurations:  92%|█████████▏| 11/12 [01:32<00:08,  8.30s/it, val_mae=0.13] /usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 🧠 Complete Insurance Cost Prediction Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Disable GPU if CUDA errors occur\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# For nice-looking progress bars in the notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('model_results', exist_ok=True)\n",
    "\n",
    "class InsuranceCostPredictor:\n",
    "    def __init__(self, data_path, model_dir='model_results'):\n",
    "        \"\"\"Initialize the predictor with data path and model directory.\"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.model_dir = model_dir\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.feature_importance = {}\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "        self.best_score = float('inf')  # Lower is better for MAE\n",
    "        \n",
    "        # Define feature categories\n",
    "        self.demographic_features = [\"Age\", \"Gender\", \"Region\", \"Number of Dependents\"]\n",
    "        self.health_features = [\"BMI\", \"Smoking Status\", \"Diabetes\", \"Hypertension\", \n",
    "                               \"Heart Disease\", \"Cancer History\", \"Stroke\", \"Liver Disease\", \n",
    "                               \"Kidney Disease\", \"COPD\", \"TB\", \"HIV/AIDS\", \n",
    "                               \"Alcohol Consumption\", \"Exercise Frequency\", \"Diet Type\", \n",
    "                               \"Stress Level\", \"Medical History Score\", \"Hospital Visits Per Year\"]\n",
    "        self.financial_features = [\"Annual Income\", \"Employment Type\", \"Credit Score\", \n",
    "                                  \"Savings Amount\", \"Previous Insurance Claims\", \n",
    "                                  \"Policy Type\", \"Policy Renewal Status\", \"Medication Costs Per Year\"]\n",
    "        \n",
    "        print(\"🔍 Insurance Cost Predictor initialized\")\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and explore the dataset.\"\"\"\n",
    "        print(\"📂 Loading dataset...\")\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "        \n",
    "        # Basic exploration\n",
    "        print(f\"📊 Dataset shape: {self.df.shape}\")\n",
    "        print(f\"🔢 Features: {self.df.shape[1]-1}\")\n",
    "        print(f\"👥 Samples: {self.df.shape[0]}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing = self.df.isnull().sum()\n",
    "        if missing.sum() > 0:\n",
    "            print(\"⚠️ Missing values detected:\")\n",
    "            print(missing[missing > 0])\n",
    "            \n",
    "            # Fill missing numerical values with median\n",
    "            for col in self.df.select_dtypes(include=['float64', 'int64']):\n",
    "                self.df[col] = self.df[col].fillna(self.df[col].median())\n",
    "            \n",
    "            # Fill missing categorical values with mode\n",
    "            for col in self.df.select_dtypes(include=['object']):\n",
    "                self.df[col] = self.df[col].fillna(self.df[col].mode()[0])\n",
    "                \n",
    "            print(\"✅ Missing values handled\")\n",
    "        else:\n",
    "            print(\"✅ No missing values detected\")\n",
    "        \n",
    "        # Target variable statistics\n",
    "        target = 'Insurance Cost'\n",
    "        print(f\"📉 Target '{target}' statistics:\")\n",
    "        print(f\"   - Mean: {self.df[target].mean():.2f}\")\n",
    "        print(f\"   - Median: {self.df[target].median():.2f}\")\n",
    "        print(f\"   - Min: {self.df[target].min():.2f}\")\n",
    "        print(f\"   - Max: {self.df[target].max():.2f}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def load_selected_features(self, features_path=None, top_n=10, use_rfecv=True):\n",
    "        \"\"\"Load features from CSV or use specific features.\"\"\"\n",
    "        if features_path:\n",
    "            print(f\"📄 Loading selected features from {features_path}...\")\n",
    "            features_df = pd.read_csv(features_path)\n",
    "            \n",
    "            # If RFECV is preferred, use only those features\n",
    "            if use_rfecv:\n",
    "                self.selected_features = features_df[features_df['RFECV_Selected'] == 1]['Feature'].tolist()\n",
    "                print(f\"✅ Loaded {len(self.selected_features)} RFECV-selected features\")\n",
    "            else:\n",
    "                # Take top N features\n",
    "                self.selected_features = features_df.head(top_n)['Feature'].tolist()\n",
    "                print(f\"✅ Loaded top {top_n} features\")\n",
    "        else:\n",
    "            # Use default important features based on previous analysis\n",
    "            self.selected_features = ['Smoking Status', 'Hypertension', 'Age', 'BMI', 'Savings Amount']\n",
    "            print(f\"✅ Using default top 5 features\")\n",
    "            \n",
    "        # Add 'Name' to excluded features to make sure it's not used\n",
    "        self.excluded_features = ['Name', 'Insurance Cost', 'BMI Smoker']\n",
    "        \n",
    "        # Print selected features with categories\n",
    "        print(\"\\n📋 Selected Features:\")\n",
    "        for i, feature in enumerate(self.selected_features, 1):\n",
    "            if feature in self.demographic_features:\n",
    "                category = \"Demographic\"\n",
    "            elif feature in self.health_features:\n",
    "                category = \"Health\"\n",
    "            elif feature in self.financial_features:\n",
    "                category = \"Financial\"\n",
    "            else:\n",
    "                category = \"Other\"\n",
    "            print(f\"   {i}. {feature} ({category})\")\n",
    "            \n",
    "        return self.selected_features\n",
    "    \n",
    "    def preprocess(self):\n",
    "        \"\"\"Preprocess the data for modeling.\"\"\"\n",
    "        print(\"\\n🔧 Preprocessing data...\")\n",
    "        \n",
    "        # Create feature engineering pipeline\n",
    "        print(\"   - Engineering features...\")\n",
    "        \n",
    "        # Add interaction terms\n",
    "        if 'BMI' in self.selected_features and 'Smoking Status' in self.selected_features:\n",
    "            self.df['BMI_Smoking'] = self.df['BMI'] * self.df['Smoking Status']\n",
    "            self.selected_features.append('BMI_Smoking')\n",
    "            print(\"     ✓ Added BMI × Smoking Status interaction\")\n",
    "            \n",
    "        if 'Age' in self.selected_features and 'Hypertension' in self.selected_features:\n",
    "            self.df['Age_Hypertension'] = self.df['Age'] * self.df['Hypertension']\n",
    "            self.selected_features.append('Age_Hypertension')\n",
    "            print(\"     ✓ Added Age × Hypertension interaction\")\n",
    "            \n",
    "        # Log transform skewed numerical features\n",
    "        skewed_features = ['Savings Amount'] \n",
    "        skewed_features = [f for f in skewed_features if f in self.selected_features]\n",
    "        \n",
    "        for feature in skewed_features:\n",
    "            if (self.df[feature] > 0).all():  # Only transform positive values\n",
    "                self.df[f'{feature}_Log'] = np.log1p(self.df[feature])\n",
    "                self.selected_features.append(f'{feature}_Log')\n",
    "                print(f\"     ✓ Log-transformed {feature}\")\n",
    "        \n",
    "        # Age groups (0-18, 19-35, 36-50, 51-65, 65+)\n",
    "        if 'Age' in self.selected_features:\n",
    "            self.df['Age_Group'] = pd.cut(\n",
    "                self.df['Age'], \n",
    "                bins=[0, 18, 35, 50, 65, 100], \n",
    "                labels=['0-18', '19-35', '36-50', '51-65', '65+']\n",
    "            )\n",
    "            self.selected_features.append('Age_Group')\n",
    "            print(\"     ✓ Created Age groups\")\n",
    "        \n",
    "        # BMI categories (Underweight, Normal, Overweight, Obese)\n",
    "        if 'BMI' in self.selected_features:\n",
    "            self.df['BMI_Category'] = pd.cut(\n",
    "                self.df['BMI'], \n",
    "                bins=[0, 18.5, 25, 30, 100], \n",
    "                labels=['Underweight', 'Normal', 'Overweight', 'Obese']\n",
    "            )\n",
    "            self.selected_features.append('BMI_Category')\n",
    "            print(\"     ✓ Created BMI categories\")\n",
    "        \n",
    "        # Prepare features and target\n",
    "        print(\"   - Preparing features and target...\")\n",
    "        X = self.df[self.selected_features].copy()\n",
    "        y = self.df['Insurance Cost']\n",
    "        \n",
    "        # Split the data\n",
    "        print(\"   - Splitting data into train/validation/test sets...\")\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val, test_size=0.25, random_state=42\n",
    "        )  # 0.25 of 0.8 = 0.2 of overall data\n",
    "        \n",
    "        print(f\"     ✓ Training set: {X_train.shape[0]} samples\")\n",
    "        print(f\"     ✓ Validation set: {X_val.shape[0]} samples\")\n",
    "        print(f\"     ✓ Test set: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        # Identify categorical features\n",
    "        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        \n",
    "        print(f\"   - Identified {len(categorical_features)} categorical and {len(numeric_features)} numerical features\")\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numeric_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "        \n",
    "        # Fit the preprocessor on the training data\n",
    "        print(\"   - Fitting preprocessing pipeline...\")\n",
    "        preprocessor.fit(X_train)\n",
    "        \n",
    "        # Transform the data\n",
    "        X_train_processed = preprocessor.transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "        X_test_processed = preprocessor.transform(X_test)\n",
    "        \n",
    "        # Save the data splits\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "        self.X_test, self.y_test = X_test, y_test\n",
    "        \n",
    "        # Save the processed data\n",
    "        self.X_train_processed = X_train_processed\n",
    "        self.X_val_processed = X_val_processed\n",
    "        self.X_test_processed = X_test_processed\n",
    "        \n",
    "        # Save the preprocessor\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "        print(\"✅ Data preprocessing complete\")\n",
    "        return X_train_processed, y_train, X_val_processed, y_val, X_test_processed, y_test\n",
    "    \n",
    "    def build_linear_model(self):\n",
    "        \"\"\"Build ElasticNet linear model.\"\"\"\n",
    "        print(\"\\n🔨 Building ElasticNet linear model...\")\n",
    "        \n",
    "        param_grid = {\n",
    "            'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "            'l1_ratio': [0.1, 0.5, 0.7, 0.9]\n",
    "        }\n",
    "        \n",
    "        elasticnet = ElasticNet(random_state=42, max_iter=2000)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=elasticnet,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"   - Starting ElasticNet hyperparameter tuning...\")\n",
    "        with tqdm(total=len(param_grid['alpha']) * len(param_grid['l1_ratio']), \n",
    "                  desc=\"   - Grid Search Progress\") as pbar:\n",
    "            # Define callback class to update progress bar\n",
    "            class TqdmCallback:\n",
    "                def __init__(self, pbar):\n",
    "                    self.pbar = pbar\n",
    "                    self.count = 0\n",
    "                    \n",
    "                def __call__(self, model, step=None):\n",
    "                    self.count += 1\n",
    "                    self.pbar.update()\n",
    "            \n",
    "            # Run grid search\n",
    "            grid_search.fit(self.X_train_processed, self.y_train)\n",
    "        \n",
    "        # Get the best model\n",
    "        best_elasticnet = grid_search.best_estimator_\n",
    "        \n",
    "        # Save the model\n",
    "        self.models['ElasticNet'] = best_elasticnet\n",
    "        \n",
    "        print(f\"   - Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"   - Best CV score: {-grid_search.best_score_:.2f} (MAE)\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_elasticnet.predict(self.X_val_processed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(self.y_val, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_val, y_pred))\n",
    "        r2 = r2_score(self.y_val, y_pred)\n",
    "        \n",
    "        # Save results\n",
    "        self.results['ElasticNet'] = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ ElasticNet model built with validation MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}\")\n",
    "        return best_elasticnet\n",
    "        \n",
    "    def build_rf_model(self):\n",
    "        \"\"\"Build Random Forest model.\"\"\"\n",
    "        print(\"\\n🔨 Building Random Forest model...\")\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "        \n",
    "        rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=rf,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"   - Starting Random Forest hyperparameter tuning...\")\n",
    "        with tqdm(total=len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']), \n",
    "                  desc=\"   - Grid Search Progress\") as pbar:\n",
    "            class TqdmCallback:\n",
    "                def __init__(self, pbar):\n",
    "                    self.pbar = pbar\n",
    "                    self.count = 0\n",
    "                    \n",
    "                def __call__(self, model, step=None):\n",
    "                    self.count += 1\n",
    "                    self.pbar.update()\n",
    "            \n",
    "            # Run grid search\n",
    "            grid_search.fit(self.X_train_processed, self.y_train)\n",
    "        \n",
    "        # Get the best model\n",
    "        best_rf = grid_search.best_estimator_\n",
    "        \n",
    "        # Save the model\n",
    "        self.models['RandomForest'] = best_rf\n",
    "        \n",
    "        print(f\"   - Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"   - Best CV score: {-grid_search.best_score_:.2f} (MAE)\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_rf.predict(self.X_val_processed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(self.y_val, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_val, y_pred))\n",
    "        r2 = r2_score(self.y_val, y_pred)\n",
    "        \n",
    "        # Save results\n",
    "        self.results['RandomForest'] = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "        \n",
    "        # Feature importance\n",
    "        if hasattr(best_rf, 'feature_importances_'):\n",
    "            # Get feature names after preprocessing\n",
    "            feature_names = []\n",
    "            for name, transformer, features in self.preprocessor.transformers_:\n",
    "                if hasattr(transformer, 'get_feature_names_out'):\n",
    "                    feature_names.extend(transformer.get_feature_names_out(features))\n",
    "                else:\n",
    "                    feature_names.extend(features)\n",
    "            \n",
    "            # Limit to length of feature_importances_\n",
    "            feature_names = feature_names[:len(best_rf.feature_importances_)]\n",
    "            \n",
    "            # Create DataFrame of feature importances\n",
    "            importances = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': best_rf.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            self.feature_importance['RandomForest'] = importances\n",
    "        \n",
    "        print(f\"✅ Random Forest model built with validation MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}\")\n",
    "        return best_rf\n",
    "    \n",
    "    def build_xgb_model(self):\n",
    "        \"\"\"Build XGBoost model.\"\"\"\n",
    "        print(\"\\n🔨 Building XGBoost model...\")\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "        \n",
    "        xgb = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=xgb,\n",
    "            param_grid=param_grid,\n",
    "            cv=5,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"   - Starting XGBoost hyperparameter tuning...\")\n",
    "        with tqdm(total=len(param_grid['n_estimators']) * len(param_grid['max_depth']) * \n",
    "                 len(param_grid['learning_rate']) * len(param_grid['subsample']), \n",
    "                 desc=\"   - Grid Search Progress\") as pbar:\n",
    "            class TqdmCallback:\n",
    "                def __init__(self, pbar):\n",
    "                    self.pbar = pbar\n",
    "                    self.count = 0\n",
    "                    \n",
    "                def __call__(self, model, step=None):\n",
    "                    self.count += 1\n",
    "                    self.pbar.update()\n",
    "            \n",
    "            # Run grid search\n",
    "            grid_search.fit(self.X_train_processed, self.y_train)\n",
    "        \n",
    "        # Get the best model\n",
    "        best_xgb = grid_search.best_estimator_\n",
    "        \n",
    "        # Save the model\n",
    "        self.models['XGBoost'] = best_xgb\n",
    "        \n",
    "        print(f\"   - Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"   - Best CV score: {-grid_search.best_score_:.2f} (MAE)\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_xgb.predict(self.X_val_processed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(self.y_val, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_val, y_pred))\n",
    "        r2 = r2_score(self.y_val, y_pred)\n",
    "        \n",
    "        # Save results\n",
    "        self.results['XGBoost'] = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "        \n",
    "        # Feature importance\n",
    "        if hasattr(best_xgb, 'feature_importances_'):\n",
    "            # Get feature names after preprocessing\n",
    "            feature_names = []\n",
    "            for name, transformer, features in self.preprocessor.transformers_:\n",
    "                if hasattr(transformer, 'get_feature_names_out'):\n",
    "                    feature_names.extend(transformer.get_feature_names_out(features))\n",
    "                else:\n",
    "                    feature_names.extend(features)\n",
    "            \n",
    "            # Limit to length of feature_importances_\n",
    "            feature_names = feature_names[:len(best_xgb.feature_importances_)]\n",
    "            \n",
    "            # Create DataFrame of feature importances\n",
    "            importances = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': best_xgb.feature_importances_\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            self.feature_importance['XGBoost'] = importances\n",
    "        \n",
    "        print(f\"✅ XGBoost model built with validation MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}\")\n",
    "        return best_xgb\n",
    "    \n",
    "    def build_nn_model(self):\n",
    "        \"\"\"Build Neural Network model.\"\"\"\n",
    "        print(\"\\n🔨 Building Neural Network model...\")\n",
    "        \n",
    "        # Define architectures to try\n",
    "        architectures = [\n",
    "            # [Dense units, dropout rate]\n",
    "            [[64, 32], [0.2, 0.2]],\n",
    "            [[128, 64, 32], [0.3, 0.2, 0.1]],\n",
    "            [[64, 64, 32, 16], [0.3, 0.3, 0.2, 0.1]]\n",
    "        ]\n",
    "        \n",
    "        learning_rates = [0.001, 0.0005]\n",
    "        batch_sizes = [32, 64]\n",
    "        \n",
    "        # Create early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=0.00001,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        best_val_mae = float('inf')\n",
    "        best_architecture = None\n",
    "        best_lr = None\n",
    "        best_batch_size = None\n",
    "        best_history = None\n",
    "        \n",
    "        # Get input shape from processed data\n",
    "        input_shape = self.X_train_processed.shape[1]\n",
    "        \n",
    "        total_combinations = len(architectures) * len(learning_rates) * len(batch_sizes)\n",
    "        pbar = tqdm(total=total_combinations, desc=\"   - Testing NN configurations\")\n",
    "        \n",
    "        for arch_idx, (units, dropouts) in enumerate(architectures):\n",
    "            for lr in learning_rates:\n",
    "                for batch_size in batch_sizes:\n",
    "                    # Build model\n",
    "                    model = Sequential()\n",
    "                    \n",
    "                    # Add input layer\n",
    "                    model.add(Dense(units[0], activation='relu', input_shape=(input_shape,)))\n",
    "                    model.add(Dropout(dropouts[0]))\n",
    "                    \n",
    "                    # Add hidden layers\n",
    "                    for i in range(1, len(units)):\n",
    "                        model.add(Dense(units[i], activation='relu'))\n",
    "                        model.add(Dropout(dropouts[i]))\n",
    "                    \n",
    "                    # Add output layer\n",
    "                    model.add(Dense(1))  # Linear activation for regression\n",
    "                    \n",
    "                    # Compile model\n",
    "                    model.compile(\n",
    "                        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                        loss=tf.keras.losses.Huber(),  # Use Huber loss (robust to outliers)\n",
    "                        metrics=['mae']\n",
    "                    )\n",
    "                    \n",
    "                    # Train model\n",
    "                    history = model.fit(\n",
    "                        self.X_train_processed, self.y_train,\n",
    "                        validation_data=(self.X_val_processed, self.y_val),\n",
    "                        epochs=100,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping, reduce_lr],\n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    # Evaluate model\n",
    "                    val_loss, val_mae = model.evaluate(\n",
    "                        self.X_val_processed, self.y_val, \n",
    "                        verbose=0\n",
    "                    )\n",
    "                    \n",
    "                    # Update progress bar with result\n",
    "                    pbar.set_postfix({'val_mae': val_mae})\n",
    "                    pbar.update()\n",
    "                    \n",
    "                    # Check if this is the best model\n",
    "                    if val_mae < best_val_mae:\n",
    "                        best_val_mae = val_mae\n",
    "                        best_architecture = (units, dropouts)\n",
    "                        best_lr = lr\n",
    "                        best_batch_size = batch_size\n",
    "                        best_history = history.history\n",
    "                        \n",
    "                        # Save the best model\n",
    "                        self.models['NeuralNetwork'] = model\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Build final model with best parameters\n",
    "        print(f\"\\n   - Best architecture: {best_architecture[0]}\")\n",
    "        print(f\"   - Best learning rate: {best_lr}\")\n",
    "        print(f\"   - Best batch size: {best_batch_size}\")\n",
    "        \n",
    "        # Get the best model\n",
    "        best_nn = self.models.get('NeuralNetwork')\n",
    "        \n",
    "        if best_nn is not None:\n",
    "            # Make predictions\n",
    "            y_pred = best_nn.predict(self.X_val_processed, verbose=0)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(self.y_val, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(self.y_val, y_pred))\n",
    "            r2 = r2_score(self.y_val, y_pred)\n",
    "            \n",
    "            # Save results\n",
    "            self.results['NeuralNetwork'] = {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'r2': r2,\n",
    "                'best_params': {\n",
    "                    'architecture': best_architecture,\n",
    "                    'learning_rate': best_lr,\n",
    "                    'batch_size': best_batch_size\n",
    "                },\n",
    "                'history': best_history\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ Neural Network model built with validation MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}\")\n",
    "            return best_nn\n",
    "        else:\n",
    "            print(\"❌ Neural Network model build failed\")\n",
    "            return None\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare the performance of all models.\"\"\"\n",
    "        print(\"\\n📊 Model Comparison:\")\n",
    "        \n",
    "        # Create a DataFrame with results\n",
    "        results_df = pd.DataFrame({\n",
    "            'Model': list(self.results.keys()),\n",
    "            'MAE': [self.results[model]['mae'] for model in self.results],\n",
    "            'RMSE': [self.results[model]['rmse'] for model in self.results],\n",
    "            'R²': [self.results[model]['r2'] for model in self.results]\n",
    "        })\n",
    "        \n",
    "        # Sort by MAE (lower is better)\n",
    "        results_df = results_df.sort_values('MAE')\n",
    "        \n",
    "        # Print results\n",
    "        print(results_df)\n",
    "        \n",
    "        # Identify best model\n",
    "        best_model_name = results_df.iloc[0]['Model']\n",
    "        self.best_model_name = best_model_name\n",
    "        self.best_model = self.models[best_model_name]\n",
    "        self.best_score = results_df.iloc[0]['MAE']\n",
    "        \n",
    "        print(f\"\\n🏆 Best model: {best_model_name} with MAE: {self.best_score:.2f}\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Visualize model performance.\"\"\"\n",
    "        print(\"\\n📈 Visualizing model results...\")\n",
    "        \n",
    "        # Create a DataFrame with results\n",
    "        results_df = pd.DataFrame({\n",
    "            'Model': list(self.results.keys()),\n",
    "            'MAE': [self.results[model]['mae'] for model in self.results],\n",
    "            'RMSE': [self.results[model]['rmse'] for model in self.results],\n",
    "            'R²': [self.results[model]['r2'] for model in self.results]\n",
    "        })\n",
    "        \n",
    "        # Sort by MAE (lower is better)\n",
    "        results_df = results_df.sort_values('MAE')\n",
    "        \n",
    "        # Create a bar chart of model performance\n",
    "        fig = make_subplots(rows=1, cols=3, \n",
    "                           subplot_titles=(\"Mean Absolute Error\", \"Root Mean Squared Error\", \"R² Score\"),\n",
    "                           shared_yaxes=True)\n",
    "        \n",
    "        # Add bars for each metric\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=results_df['Model'], y=results_df['MAE'], name='MAE',\n",
    "                   text=results_df['MAE'].round(2), textposition='auto',\n",
    "                   marker_color='crimson'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=results_df['Model'], y=results_df['RMSE'], name='RMSE',\n",
    "                   text=results_df['RMSE'].round(2), textposition='auto',\n",
    "                   marker_color='darkorange'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=results_df['Model'], y=results_df['R²'], name='R²',\n",
    "                   text=results_df['R²'].round(4), textposition='auto',\n",
    "                   marker_color='teal'),\n",
    "            row=1, col=3\n",
    "        )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title='Model Performance Comparison',\n",
    "            height=500,\n",
    "            width=1000,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "        \n",
    "        # If Neural Network is in the models, plot training history\n",
    "        if 'NeuralNetwork' in self.results and 'history' in self.results['NeuralNetwork']:\n",
    "            history = self.results['NeuralNetwork']['history']\n",
    "            \n",
    "            # Create a line plot of training history\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=list(range(len(history['loss']))), y=history['loss'],\n",
    "                          name='Training Loss', line=dict(color='blue'))\n",
    "            )\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=list(range(len(history['val_loss']))), y=history['val_loss'],\n",
    "                          name='Validation Loss', line=dict(color='red'))\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title='Neural Network Training History',\n",
    "                xaxis_title='Epochs',\n",
    "                yaxis_title='Loss',\n",
    "                height=400,\n",
    "                width=800\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "        # Plot feature importances if available\n",
    "        if self.feature_importance:\n",
    "            model_with_importances = next(iter(self.feature_importance.keys()))\n",
    "            importances = self.feature_importance[model_with_importances]\n",
    "            \n",
    "            # Take top 15 features\n",
    "            importances = importances.head(15)\n",
    "            \n",
    "            fig = px.bar(\n",
    "                importances, \n",
    "                x='Importance', \n",
    "                y='Feature',\n",
    "                orientation='h',\n",
    "                title=f'Top 15 Feature Importances ({model_with_importances})',\n",
    "                color='Importance',\n",
    "                color_continuous_scale='teal'\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                height=500, \n",
    "                width=800,\n",
    "                yaxis=dict(autorange=\"reversed\")\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "        \n",
    "        # Actual vs. Predicted plot for best model\n",
    "        if self.best_model_name:\n",
    "            # Get predictions\n",
    "            if self.best_model_name == 'NeuralNetwork':\n",
    "                y_pred = self.models[self.best_model_name].predict(self.X_test_processed, verbose=0)\n",
    "            else:\n",
    "                y_pred = self.models[self.best_model_name].predict(self.X_test_processed)\n",
    "            \n",
    "            # Create a scatter plot of actual vs. predicted\n",
    "            fig = px.scatter(\n",
    "                x=self.y_test, \n",
    "                y=y_pred.flatten(),\n",
    "                title=f'Actual vs. Predicted Insurance Cost ({self.best_model_name})',\n",
    "                labels={'x': 'Actual Cost', 'y': 'Predicted Cost'}\n",
    "            )\n",
    "            \n",
    "            # Add a 45-degree line\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[self.y_test.min(), self.y_test.max()], \n",
    "                    y=[self.y_test.min(), self.y_test.max()],\n",
    "                    mode='lines',\n",
    "                    name='45° Line',\n",
    "                    line=dict(color='red', dash='dash')\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(height=600, width=800)\n",
    "            fig.show()\n",
    "            \n",
    "            # Residual plot\n",
    "            residuals = self.y_test - y_pred.flatten()\n",
    "            fig = px.scatter(\n",
    "                x=y_pred.flatten(), \n",
    "                y=residuals,\n",
    "                title=f'Residual Plot ({self.best_model_name})',\n",
    "                labels={'x': 'Predicted Cost', 'y': 'Residuals'}\n",
    "            )\n",
    "            \n",
    "            # Add a horizontal line at y=0\n",
    "            fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\")\n",
    "            \n",
    "            fig.update_layout(height=500, width=800)\n",
    "            fig.show()\n",
    "            \n",
    "            # Histogram of residuals\n",
    "            fig = px.histogram(\n",
    "                residuals, \n",
    "                nbins=50,\n",
    "                title=f'Distribution of Residuals ({self.best_model_name})'\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(height=400, width=800)\n",
    "            fig.show()\n",
    "    \n",
    "    def final_evaluation(self):\n",
    "        \"\"\"Evaluate the best model on the test set.\"\"\"\n",
    "        if self.best_model is None:\n",
    "            print(\"❌ No best model found. Run compare_models() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n🧪 Final evaluation of {self.best_model_name} on test data...\")\n",
    "        \n",
    "        # Get predictions\n",
    "        if self.best_model_name == 'NeuralNetwork':\n",
    "            y_pred = self.best_model.predict(self.X_test_processed, verbose=0)\n",
    "        else:\n",
    "            y_pred = self.best_model.predict(self.X_test_processed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(self.y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "        \n",
    "        # Save test results\n",
    "        self.test_results = {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2\n",
    "        }\n",
    "        \n",
    "        print(f\"📊 Test Set Results:\")\n",
    "        print(f\"   - MAE: {mae:.2f}\")\n",
    "        print(f\"   - RMSE: {rmse:.2f}\")\n",
    "        print(f\"   - R²: {r2:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        model_path = f\"{self.model_dir}/{self.best_model_name}.pkl\"\n",
    "        if self.best_model_name != 'NeuralNetwork':\n",
    "            joblib.dump(self.best_model, model_path)\n",
    "            print(f\"✅ Best model saved to {model_path}\")\n",
    "        else:\n",
    "            # Save neural network model\n",
    "            model_path = f\"{self.model_dir}/{self.best_model_name}.h5\"\n",
    "            self.best_model.save(model_path)\n",
    "            print(f\"✅ Neural Network model saved to {model_path}\")\n",
    "        \n",
    "        # Save the preprocessor\n",
    "        preprocessor_path = f\"{self.model_dir}/preprocessor.pkl\"\n",
    "        joblib.dump(self.preprocessor, preprocessor_path)\n",
    "        print(f\"✅ Preprocessor saved to {preprocessor_path}\")\n",
    "        \n",
    "        # Save results\n",
    "        results_path = f\"{self.model_dir}/results.csv\"\n",
    "        results_df = pd.DataFrame({\n",
    "            'Model': list(self.results.keys()) + ['Best Model (Test)'],\n",
    "            'MAE': [self.results[model]['mae'] for model in self.results] + [mae],\n",
    "            'RMSE': [self.results[model]['rmse'] for model in self.results] + [rmse],\n",
    "            'R²': [self.results[model]['r2'] for model in self.results] + [r2]\n",
    "        })\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"✅ Results saved to {results_path}\")\n",
    "        \n",
    "        return self.test_results\n",
    "    \n",
    "    def run_pipeline(self, features_path=None, top_n=10, use_rfecv=True):\n",
    "        \"\"\"Run the full modeling pipeline.\"\"\"\n",
    "        print(\"🚀 Starting Insurance Cost Prediction Pipeline...\")\n",
    "        \n",
    "        # Step 1: Load data\n",
    "        self.load_data()\n",
    "        \n",
    "        # Step 2: Load selected features\n",
    "        self.load_selected_features(features_path, top_n, use_rfecv)\n",
    "        \n",
    "        # Step 3: Preprocess data\n",
    "        self.preprocess()\n",
    "        \n",
    "        # Step 4: Build models\n",
    "        print(\"\\n🛠️ Building models...\")\n",
    "        self.build_linear_model()\n",
    "        self.build_rf_model()\n",
    "        self.build_xgb_model()\n",
    "        self.build_nn_model()\n",
    "        \n",
    "        # Step 5: Compare models\n",
    "        self.compare_models()\n",
    "        \n",
    "        # Step 6: Visualize results\n",
    "        self.visualize_results()\n",
    "        \n",
    "        # Step 7: Final evaluation\n",
    "        self.final_evaluation()\n",
    "        \n",
    "        print(\"\\n✅ Pipeline complete!\")\n",
    "        return self.best_model_name, self.test_results\n",
    "\n",
    "    def export_all_models(self):\n",
    "        \"\"\"Export all trained models to disk.\"\"\"\n",
    "        print(\"\\n💾 Exporting all trained models...\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "        \n",
    "        # Export each model\n",
    "        for model_name, model in self.models.items():\n",
    "            if model_name == 'NeuralNetwork':\n",
    "                # Save neural network model in h5 format\n",
    "                model_path = f\"{self.model_dir}/{model_name}.h5\"\n",
    "                model.save(model_path)\n",
    "            else:\n",
    "                # Save scikit-learn models with joblib\n",
    "                model_path = f\"{self.model_dir}/{model_name}.pkl\"\n",
    "                joblib.dump(model, model_path)\n",
    "            \n",
    "            # Print success message with metrics\n",
    "            if model_name in self.results:\n",
    "                mae = self.results[model_name]['mae']\n",
    "                rmse = self.results[model_name]['rmse']\n",
    "                r2 = self.results[model_name]['r2']\n",
    "                print(f\"✅ {model_name} saved to {model_path} (MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f})\")\n",
    "            else:\n",
    "                print(f\"✅ {model_name} saved to {model_path}\")\n",
    "        \n",
    "        # Also export the preprocessor separately (if not already done)\n",
    "        preprocessor_path = f\"{self.model_dir}/preprocessor.pkl\"\n",
    "        joblib.dump(self.preprocessor, preprocessor_path)\n",
    "        print(f\"✅ Preprocessor saved to {preprocessor_path}\")\n",
    "        \n",
    "        return list(self.models.keys())\n",
    "# Run the pipeline\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = InsuranceCostPredictor(data_path=\"insurance_processed_20250321_163523.csv\")\n",
    "    predictor.run_pipeline(features_path=\"selected_features.csv\", top_n=10, use_rfecv=True)\n",
    "    \n",
    "    # Export all models\n",
    "    predictor.export_all_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
